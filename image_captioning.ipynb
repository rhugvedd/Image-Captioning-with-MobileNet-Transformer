{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-11-16T12:20:51.475456Z",
     "iopub.status.busy": "2023-11-16T12:20:51.475176Z",
     "iopub.status.idle": "2023-11-16T12:20:54.774381Z",
     "shell.execute_reply": "2023-11-16T12:20:54.773666Z"
    },
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:20:54.778897Z",
     "iopub.status.busy": "2023-11-16T12:20:54.778464Z",
     "iopub.status.idle": "2023-11-16T12:20:54.785934Z",
     "shell.execute_reply": "2023-11-16T12:20:54.785221Z"
    },
    "id": "kaNy_l7tGuAZ"
   },
   "outputs": [],
   "source": [
    "def flickr8k(path='flickr8k'):\n",
    "  path = pathlib.Path(path)\n",
    "\n",
    "  if len(list(path.rglob('*'))) < 16197:\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "        cache_dir='.',\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "        cache_dir='.',\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    \n",
    "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "  captions = (line.split('\\t') for line in captions)\n",
    "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "\n",
    "  cap_dict = collections.defaultdict(list)\n",
    "  for fname, cap in captions:\n",
    "    cap_dict[fname].append(cap)\n",
    "\n",
    "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "\n",
    "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "\n",
    "  train_ds = tf.data.experimental.from_list(train_captions)\n",
    "  test_ds = tf.data.experimental.from_list(test_captions)\n",
    "\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQICBAF4FmSL"
   },
   "source": [
    "#### Conceptual Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:20:54.789337Z",
     "iopub.status.busy": "2023-11-16T12:20:54.789048Z",
     "iopub.status.idle": "2023-11-16T12:20:54.798502Z",
     "shell.execute_reply": "2023-11-16T12:20:54.797863Z"
    },
    "id": "vQwnxXZXRl12"
   },
   "outputs": [],
   "source": [
    "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
    "  def iter_index(index_path):\n",
    "    with open(index_path) as f:\n",
    "      for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        yield caption, url\n",
    "\n",
    "  def download_image_urls(data_dir, urls):\n",
    "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "    def save_image(url):\n",
    "      hash = hashlib.sha1(url.encode())\n",
    "      # Name the files after the hash of the URL.\n",
    "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
    "      if file_path.exists():\n",
    "        # Only download each file once.\n",
    "        return file_path\n",
    "\n",
    "      try:\n",
    "        result = requests.get(url, timeout=5)\n",
    "      except Exception:\n",
    "        file_path = None\n",
    "      else:\n",
    "        file_path.write_bytes(result.content)\n",
    "      return file_path\n",
    "    \n",
    "    result = []\n",
    "    out_paths = ex.map(save_image, urls)\n",
    "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
    "      result.append(file_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "  def ds_from_index_file(index_path, data_dir, count):\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    index = list(itertools.islice(iter_index(index_path), count))\n",
    "    captions = [caption for caption, url in index]\n",
    "    urls = [url for caption, url in index]\n",
    "\n",
    "    paths = download_image_urls(data_dir, urls)\n",
    "\n",
    "    new_captions = []\n",
    "    new_paths = []\n",
    "    for cap, path in zip(captions, paths):\n",
    "      if path is None:\n",
    "        # Download failed, so skip this pair.\n",
    "        continue\n",
    "      new_captions.append(cap)\n",
    "      new_paths.append(path)\n",
    "    \n",
    "    new_paths = [str(p) for p in new_paths]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
    "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
    "    return ds\n",
    "\n",
    "  data_dir = pathlib.Path(data_dir)\n",
    "  train_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
    "    cache_subdir=data_dir,\n",
    "    cache_dir='.')\n",
    "  \n",
    "  val_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
    "    cache_subdir=data_dir,\n",
    "    cache_dir='.')\n",
    "  \n",
    "  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n",
    "  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n",
    "\n",
    "  return train_raw, test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBAagBw5p-TM"
   },
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFtTZaobquNr"
   },
   "source": [
    "The Flickr8k is a good choice because it contains 5-captions per image, more data for a smaller download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:20:54.801898Z",
     "iopub.status.busy": "2023-11-16T12:20:54.801401Z",
     "iopub.status.idle": "2023-11-16T12:21:08.695726Z",
     "shell.execute_reply": "2023-11-16T12:21:08.694922Z"
    },
    "id": "EJySPbzJ4Wxw"
   },
   "outputs": [],
   "source": [
    "choose = 'flickr8k'\n",
    "\n",
    "if choose == 'flickr8k':\n",
    "  train_raw, test_raw = flickr8k()\n",
    "else:\n",
    "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:08.709444Z",
     "iopub.status.busy": "2023-11-16T12:21:08.709175Z",
     "iopub.status.idle": "2023-11-16T12:21:09.863056Z",
     "shell.execute_reply": "2023-11-16T12:21:09.862232Z"
    },
    "id": "xIa0ZaP4tBez"
   },
   "outputs": [],
   "source": [
    "for ex_path, ex_captions in train_raw.take(1):\n",
    "  print(ex_path)\n",
    "  print(ex_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "### Image feature extractor\n",
    "\n",
    "We will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so we can use the last layer of feature-maps:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:09.867270Z",
     "iopub.status.busy": "2023-11-16T12:21:09.866999Z",
     "iopub.status.idle": "2023-11-16T12:21:11.455372Z",
     "shell.execute_reply": "2023-11-16T12:21:11.454659Z"
    },
    "id": "IlUckK8Zfikv"
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE=(224, 224, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=True)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:11.459569Z",
     "iopub.status.busy": "2023-11-16T12:21:11.459286Z",
     "iopub.status.idle": "2023-11-16T12:21:11.463337Z",
     "shell.execute_reply": "2023-11-16T12:21:11.462736Z"
    },
    "id": "zXR0217aRPFR"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JyQ7zS6gzZh"
   },
   "source": [
    "The model returns a feature map for each image in the input batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:11.466587Z",
     "iopub.status.busy": "2023-11-16T12:21:11.466366Z",
     "iopub.status.idle": "2023-11-16T12:21:12.637941Z",
     "shell.execute_reply": "2023-11-16T12:21:12.637202Z"
    },
    "id": "sY86n2i6wJNm"
   },
   "outputs": [],
   "source": [
    "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
    "\n",
    "print(test_img_batch.shape)\n",
    "print(mobilenet(test_img_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:12.641849Z",
     "iopub.status.busy": "2023-11-16T12:21:12.641325Z",
     "iopub.status.idle": "2023-11-16T12:21:12.645242Z",
     "shell.execute_reply": "2023-11-16T12:21:12.644582Z"
    },
    "id": "NroZIzB90hD3"
   },
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "  s = tf.strings.lower(s)\n",
    "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:12.648567Z",
     "iopub.status.busy": "2023-11-16T12:21:12.648016Z",
     "iopub.status.idle": "2023-11-16T12:21:12.659134Z",
     "shell.execute_reply": "2023-11-16T12:21:12.658489Z"
    },
    "id": "n9SQOXFsyS36"
   },
   "outputs": [],
   "source": [
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    ragged=True)\n",
    "# Learn the vocabulary from the caption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:12.662348Z",
     "iopub.status.busy": "2023-11-16T12:21:12.661835Z",
     "iopub.status.idle": "2023-11-16T12:21:14.093306Z",
     "shell.execute_reply": "2023-11-16T12:21:14.092405Z"
    },
    "id": "oJGE34aiRPFo"
   },
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:14.798785Z",
     "iopub.status.busy": "2023-11-16T12:21:14.798515Z",
     "iopub.status.idle": "2023-11-16T12:21:14.881656Z",
     "shell.execute_reply": "2023-11-16T12:21:14.881017Z"
    },
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indices to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:14.898716Z",
     "iopub.status.busy": "2023-11-16T12:21:14.898158Z",
     "iopub.status.idle": "2023-11-16T12:21:14.957827Z",
     "shell.execute_reply": "2023-11-16T12:21:14.957178Z"
    },
    "id": "rrUUfGc65vAT"
   },
   "outputs": [],
   "source": [
    "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "Prepare the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:14.961405Z",
     "iopub.status.busy": "2023-11-16T12:21:14.961158Z",
     "iopub.status.idle": "2023-11-16T12:21:14.965141Z",
     "shell.execute_reply": "2023-11-16T12:21:14.964467Z"
    },
    "id": "3_Lqwl9NiGT0"
   },
   "outputs": [],
   "source": [
    "def match_shapes(images, captions):\n",
    "  caption_shape = einops.parse_shape(captions, 'b c')\n",
    "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
    "  images = einops.repeat(\n",
    "      images, 'b ... -> (b c) ...',\n",
    "      c = caption_shape['c'])\n",
    "  return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:14.968239Z",
     "iopub.status.busy": "2023-11-16T12:21:14.967756Z",
     "iopub.status.idle": "2023-11-16T12:21:16.109934Z",
     "shell.execute_reply": "2023-11-16T12:21:16.109151Z"
    },
    "id": "CZGUsuGzUfzt"
   },
   "outputs": [],
   "source": [
    "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
    "  break\n",
    "\n",
    "print('image paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)\n",
    "print()\n",
    "\n",
    "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
    "\n",
    "print('image_paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:16.113368Z",
     "iopub.status.busy": "2023-11-16T12:21:16.113080Z",
     "iopub.status.idle": "2023-11-16T12:21:16.116965Z",
     "shell.execute_reply": "2023-11-16T12:21:16.116257Z"
    },
    "id": "2DsgQ_hZT4C2"
   },
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:16.120096Z",
     "iopub.status.busy": "2023-11-16T12:21:16.119825Z",
     "iopub.status.idle": "2023-11-16T12:21:16.125109Z",
     "shell.execute_reply": "2023-11-16T12:21:16.124439Z"
    },
    "id": "4_Pt9zldjQ0q"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "\n",
    "  return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQ85t1GNfpQ"
   },
   "source": [
    "You could install the feature extractor in your model and train on the datasets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:16.127968Z",
     "iopub.status.busy": "2023-11-16T12:21:16.127741Z",
     "iopub.status.idle": "2023-11-16T12:21:17.379813Z",
     "shell.execute_reply": "2023-11-16T12:21:17.379052Z"
    },
    "id": "1KlhOG5cjQ0r"
   },
   "outputs": [],
   "source": [
    "train_ds = prepare_dataset(train_raw, tokenizer)\n",
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:17.383155Z",
     "iopub.status.busy": "2023-11-16T12:21:17.382890Z",
     "iopub.status.idle": "2023-11-16T12:21:17.521874Z",
     "shell.execute_reply": "2023-11-16T12:21:17.521199Z"
    },
    "id": "d7Zy9F3zX7i2"
   },
   "outputs": [],
   "source": [
    "test_ds = prepare_dataset(test_raw, tokenizer)\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:17.525269Z",
     "iopub.status.busy": "2023-11-16T12:21:17.525014Z",
     "iopub.status.idle": "2023-11-16T12:21:17.532932Z",
     "shell.execute_reply": "2023-11-16T12:21:17.532330Z"
    },
    "id": "9N1MX5ym6xm5"
   },
   "outputs": [],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "\n",
    "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:17.535634Z",
     "iopub.status.busy": "2023-11-16T12:21:17.535396Z",
     "iopub.status.idle": "2023-11-16T12:21:45.706530Z",
     "shell.execute_reply": "2023-11-16T12:21:45.705781Z"
    },
    "id": "tNdzrenxB3Yy"
   },
   "outputs": [],
   "source": [
    "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
    "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798DtfH51UI8"
   },
   "source": [
    " </section>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:45.711256Z",
     "iopub.status.busy": "2023-11-16T12:21:45.710624Z",
     "iopub.status.idle": "2023-11-16T12:21:45.817993Z",
     "shell.execute_reply": "2023-11-16T12:21:45.817354Z"
    },
    "id": "Pwic2YCjHZmV"
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:45.821752Z",
     "iopub.status.busy": "2023-11-16T12:21:45.821067Z",
     "iopub.status.idle": "2023-11-16T12:21:45.825654Z",
     "shell.execute_reply": "2023-11-16T12:21:45.825026Z"
    },
    "id": "3B80JXj7HloX"
   },
   "outputs": [],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jfb8qknlsKi"
   },
   "source": [
    "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:45.828769Z",
     "iopub.status.busy": "2023-11-16T12:21:45.828288Z",
     "iopub.status.idle": "2023-11-16T12:21:46.046201Z",
     "shell.execute_reply": "2023-11-16T12:21:46.045498Z"
    },
    "id": "YJBEwuXLZQdw"
   },
   "outputs": [],
   "source": [
    "for (inputs, ex_labels) in train_ds.take(1):\n",
    "  (ex_img, ex_in_tok) = inputs\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_in_tok.shape)\n",
    "print(ex_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.059224Z",
     "iopub.status.busy": "2023-11-16T12:21:46.058730Z",
     "iopub.status.idle": "2023-11-16T12:21:46.064123Z",
     "shell.execute_reply": "2023-11-16T12:21:46.063534Z"
    },
    "id": "P91LU2F0a9Ga"
   },
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "    \n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.067743Z",
     "iopub.status.busy": "2023-11-16T12:21:46.067204Z",
     "iopub.status.idle": "2023-11-16T12:21:46.071917Z",
     "shell.execute_reply": "2023-11-16T12:21:46.071336Z"
    },
    "id": "6JTLiX3lKooQ"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.075191Z",
     "iopub.status.busy": "2023-11-16T12:21:46.074713Z",
     "iopub.status.idle": "2023-11-16T12:21:46.079584Z",
     "shell.execute_reply": "2023-11-16T12:21:46.079004Z"
    },
    "id": "rIY6Vu2pLBAO"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "    \n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.082765Z",
     "iopub.status.busy": "2023-11-16T12:21:46.082249Z",
     "iopub.status.idle": "2023-11-16T12:21:46.087397Z",
     "shell.execute_reply": "2023-11-16T12:21:46.086759Z"
    },
    "id": "cWKrl7teOnH2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.090637Z",
     "iopub.status.busy": "2023-11-16T12:21:46.090152Z",
     "iopub.status.idle": "2023-11-16T12:21:46.095465Z",
     "shell.execute_reply": "2023-11-16T12:21:46.094883Z"
    },
    "id": "ydcW5KZZHou7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "      \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "    \n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.098844Z",
     "iopub.status.busy": "2023-11-16T12:21:46.098368Z",
     "iopub.status.idle": "2023-11-16T12:21:46.106292Z",
     "shell.execute_reply": "2023-11-16T12:21:46.105685Z"
    },
    "id": "CeWw2SFDHUfo"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzQHqANd1A6Q"
   },
   "source": [
    "The smart initialization will significantly reduce the initial loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:46.109406Z",
     "iopub.status.busy": "2023-11-16T12:21:46.108929Z",
     "iopub.status.idle": "2023-11-16T12:21:48.770506Z",
     "shell.execute_reply": "2023-11-16T12:21:48.769841Z"
    },
    "id": "GGnOQyc501B2"
   },
   "outputs": [],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gou4fPH_SWgH"
   },
   "source": [
    "To build the model, you need to combine several parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:48.774519Z",
     "iopub.status.busy": "2023-11-16T12:21:48.773793Z",
     "iopub.status.idle": "2023-11-16T12:21:48.780283Z",
     "shell.execute_reply": "2023-11-16T12:21:48.779717Z"
    },
    "id": "bHCISYehH1f6"
   },
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:48.783760Z",
     "iopub.status.busy": "2023-11-16T12:21:48.783291Z",
     "iopub.status.idle": "2023-11-16T12:21:48.788511Z",
     "shell.execute_reply": "2023-11-16T12:21:48.787918Z"
    },
    "id": "lPdb7I4h9Ulo"
   },
   "outputs": [],
   "source": [
    "  @Captioner.add_method\n",
    "  def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "\n",
    "    if image.shape[-1] == 3:\n",
    "      # Apply the feature-extractor, if you get an RGB image.\n",
    "      image = self.feature_extractor(image)\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "\n",
    "\n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "      \n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:48.791791Z",
     "iopub.status.busy": "2023-11-16T12:21:48.791275Z",
     "iopub.status.idle": "2023-11-16T12:21:48.903568Z",
     "shell.execute_reply": "2023-11-16T12:21:48.902898Z"
    },
    "id": "kmM7aZQsLiyU"
   },
   "outputs": [],
   "source": [
    "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:49.185747Z",
     "iopub.status.busy": "2023-11-16T12:21:49.185127Z",
     "iopub.status.idle": "2023-11-16T12:21:49.191210Z",
     "shell.execute_reply": "2023-11-16T12:21:49.190608Z"
    },
    "id": "Nf1Jie9ef_Cg"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=1):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:55.196365Z",
     "iopub.status.busy": "2023-11-16T12:21:55.196081Z",
     "iopub.status.idle": "2023-11-16T12:21:55.201448Z",
     "shell.execute_reply": "2023-11-16T12:21:55.200789Z"
    },
    "id": "s24im3FqxAfT"
   },
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:55.204614Z",
     "iopub.status.busy": "2023-11-16T12:21:55.204350Z",
     "iopub.status.idle": "2023-11-16T12:21:55.209096Z",
     "shell.execute_reply": "2023-11-16T12:21:55.208436Z"
    },
    "id": "IKDwbZOCZ-AP"
   },
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "    self.image = load_image(image_path)\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAxp4KZRKDk9"
   },
   "source": [
    "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:59.554403Z",
     "iopub.status.busy": "2023-11-16T12:21:59.554122Z",
     "iopub.status.idle": "2023-11-16T12:21:59.561762Z",
     "shell.execute_reply": "2023-11-16T12:21:59.561149Z"
    },
    "id": "MjzrwGZp23xx"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBXG0dCDKO55"
   },
   "source": [
    "Configure and execute the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:59.565305Z",
     "iopub.status.busy": "2023-11-16T12:21:59.564780Z",
     "iopub.status.idle": "2023-11-16T12:21:59.591913Z",
     "shell.execute_reply": "2023-11-16T12:21:59.591301Z"
    },
    "id": "2OR5ZpAII__u"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "           loss=masked_loss,\n",
    "           metrics=[masked_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro955bQ2KR0X"
   },
   "source": [
    "Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:21:59.595353Z",
     "iopub.status.busy": "2023-11-16T12:21:59.594833Z",
     "iopub.status.idle": "2023-11-16T12:26:59.062911Z",
     "shell.execute_reply": "2023-11-16T12:26:59.062172Z"
    },
    "id": "3aB0baOVMZe9"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    steps_per_epoch=800,\n",
    "    validation_data=test_ds.repeat(),\n",
    "    validation_steps=200,\n",
    "    epochs=1,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P634LfVgw-eV"
   },
   "source": [
    "Plot the loss and accuracy over the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:26:59.066886Z",
     "iopub.status.busy": "2023-11-16T12:26:59.066632Z",
     "iopub.status.idle": "2023-11-16T12:26:59.244365Z",
     "shell.execute_reply": "2023-11-16T12:26:59.243694Z"
    },
    "id": "6Wn8KSkUw916"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:26:59.247887Z",
     "iopub.status.busy": "2023-11-16T12:26:59.247387Z",
     "iopub.status.idle": "2023-11-16T12:26:59.413984Z",
     "shell.execute_reply": "2023-11-16T12:26:59.413339Z"
    },
    "id": "yZQ78b2Kxw-T"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:26:59.417099Z",
     "iopub.status.busy": "2023-11-16T12:26:59.416712Z",
     "iopub.status.idle": "2023-11-16T12:27:00.189526Z",
     "shell.execute_reply": "2023-11-16T12:27:00.188834Z"
    },
    "id": "1UQPtNTb2eu3"
   },
   "outputs": [],
   "source": [
    "result = model.simple_gen(image, temperature=0.0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.193458Z",
     "iopub.status.busy": "2023-11-16T12:27:00.192817Z",
     "iopub.status.idle": "2023-11-16T12:27:00.196155Z",
     "shell.execute_reply": "2023-11-16T12:27:00.195562Z"
    },
    "id": "zHKOpm0w5Xto"
   },
   "outputs": [],
   "source": [
    "str_tokens = result.split()\n",
    "str_tokens.append('[END]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.199446Z",
     "iopub.status.busy": "2023-11-16T12:27:00.198911Z",
     "iopub.status.idle": "2023-11-16T12:27:00.203499Z",
     "shell.execute_reply": "2023-11-16T12:27:00.202906Z"
    },
    "id": "XZpyuQvq2q-B"
   },
   "outputs": [],
   "source": [
    "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
    "[map.shape for map in attn_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.206793Z",
     "iopub.status.busy": "2023-11-16T12:27:00.206272Z",
     "iopub.status.idle": "2023-11-16T12:27:00.213997Z",
     "shell.execute_reply": "2023-11-16T12:27:00.213379Z"
    },
    "id": "ojwtvnkh6mS-"
   },
   "outputs": [],
   "source": [
    "attention_maps = tf.concat(attn_maps, axis=0)\n",
    "attention_maps = einops.reduce(\n",
    "    attention_maps,\n",
    "    'batch heads sequence (height width) -> sequence height width',\n",
    "    height=7, width=7,\n",
    "    reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.217301Z",
     "iopub.status.busy": "2023-11-16T12:27:00.216892Z",
     "iopub.status.idle": "2023-11-16T12:27:00.223070Z",
     "shell.execute_reply": "2023-11-16T12:27:00.222480Z"
    },
    "id": "ASWmWerGCZp3"
   },
   "outputs": [],
   "source": [
    "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to plot the Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.226481Z",
     "iopub.status.busy": "2023-11-16T12:27:00.225922Z",
     "iopub.status.idle": "2023-11-16T12:27:00.231004Z",
     "shell.execute_reply": "2023-11-16T12:27:00.230361Z"
    },
    "id": "fD_y7PD6RPGt"
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(image, str_tokens, attention_map):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "    len_result = len(str_tokens)\n",
    "    \n",
    "    titles = []\n",
    "    for i in range(len_result):\n",
    "      map = attention_map[i]\n",
    "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "      ax = fig.add_subplot(3, grid_size, i+1)\n",
    "      titles.append(ax.set_title(str_tokens[i]))\n",
    "      img = ax.imshow(image)\n",
    "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
    "                clim=[0.0, np.max(map)])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:00.234182Z",
     "iopub.status.busy": "2023-11-16T12:27:00.233615Z",
     "iopub.status.idle": "2023-11-16T12:27:02.082948Z",
     "shell.execute_reply": "2023-11-16T12:27:02.082231Z"
    },
    "id": "PI4NAAws9rvY"
   },
   "outputs": [],
   "source": [
    "plot_attention_maps(image/255, str_tokens, attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riTz0abQKMkV"
   },
   "source": [
    "A function to show the attention plots of a particular image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:02.094012Z",
     "iopub.status.busy": "2023-11-16T12:27:02.093520Z",
     "iopub.status.idle": "2023-11-16T12:27:02.098795Z",
     "shell.execute_reply": "2023-11-16T12:27:02.098187Z"
    },
    "id": "mktpfW-SKQIJ"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def run_and_show_attention(self, image, temperature=0.0):\n",
    "  result_txt = self.simple_gen(image, temperature)\n",
    "  str_tokens = result_txt.split()\n",
    "  str_tokens.append('[END]')\n",
    "\n",
    "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
    "  attention_maps = tf.concat(attention_maps, axis=0)\n",
    "  attention_maps = einops.reduce(\n",
    "      attention_maps,\n",
    "      'batch heads sequence (height width) -> sequence height width',\n",
    "      height=7, width=7,\n",
    "      reduction='mean')\n",
    "  \n",
    "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
    "  t = plt.suptitle(result_txt)\n",
    "  t.set_y(1.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rprk3HEvZuxb"
   },
   "source": [
    "Try on specific images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T12:27:04.719271Z",
     "iopub.status.busy": "2023-11-16T12:27:04.719013Z",
     "iopub.status.idle": "2023-11-16T12:27:06.979708Z",
     "shell.execute_reply": "2023-11-16T12:27:06.978967Z"
    },
    "id": "9Psd1quzaAWg"
   },
   "outputs": [],
   "source": [
    "image = load_image('./Images/Image1.png')\n",
    "\n",
    "run_and_show_attention(model, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
